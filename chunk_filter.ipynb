{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5581792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e7637",
   "metadata": {},
   "source": [
    "# Chunking & Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717fe22",
   "metadata": {},
   "source": [
    "The datasets are too large to import directly into Google Sheets. I'm using Python to import the taxi files via chunks, filtering the chunks, and then concantinating the results to create datasets of rides between 4/14/2014 - 4/20/2014.\n",
    "\n",
    "The progress bar was added for reference, as this process took several minutes. There were also issues with low memory that needed to be monitored. Steps were taken below to reduce memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ff864",
   "metadata": {},
   "source": [
    "### Yellow and Green taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e685e73c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing: 159: 100%|████████████████████████████████████████████████████████████████| 159/159 [01:11<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# Code adapted from: https://stackoverflow.com/a/66519332\n",
    "# Troubleshooting help courtesy of Alex Baransky\n",
    "\n",
    "# For counting rows\n",
    "def row_count(filename):\n",
    "    with open(filename) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i\n",
    "\n",
    "filename = 'green_taxi.csv'\n",
    "# filename = 'yellow_taxi.csv'\n",
    "\n",
    "use_cols = ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'trip_distance', \n",
    "            'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n",
    "           'tip_amount', 'total_amount']\n",
    "# dtype_dict = {'vendor_id': 'object', 'trip_distance': 'float', 'pickup_longitude': 'float', 'pickup_latitude': 'float',\n",
    "#               'dropoff_longitude': 'float', 'dropoff_latitude': 'float', 'tip_amount': 'float', 'total_amount': 'float'}\n",
    "\n",
    "start_date = pd.to_datetime('04/14/2014 12:00:00 AM')\n",
    "end_date = pd.to_datetime('04/20/2014 11:59:59 PM')\n",
    "\n",
    "num_rows = row_count(filename) # Find total rows in data\n",
    "# num_rows = 165114361 # Found it by running the above function, takes long!\n",
    "chunk_size = 1e5\n",
    "temp = pd.read_csv(filename, nrows=0)\n",
    "temp.columns = temp.columns.str.lower().str.replace('vendorid', 'vendor_id')\n",
    "\n",
    "df_list = [temp[:0]] # We want to make a list of chunk dfs which we will collapse (concat) later, this is just an empty\n",
    "                        # df for the colnames\n",
    "    \n",
    "t = math.ceil(num_rows/chunk_size) # This isn't really important, something to do with progress bar\n",
    "\n",
    "with tqdm(total = t, file = sys.stdout) as pbar: # Some more progress bar stuff?\n",
    "\n",
    "    # Below is iterating over the chunks\n",
    "    for i, chunk in enumerate(pd.read_csv(filename, chunksize=chunk_size, low_memory=False,\n",
    "                                          usecols = use_cols, names = temp.columns)): # dtype = dtype_dict,\n",
    "\n",
    "        chunk['pickup_datetime'] = pd.to_datetime(chunk['pickup_datetime'], errors='coerce', format = '%m/%d/%Y %I:%M:%S %p')\n",
    "        \n",
    "        # We wrote the bottom 2 lines, creating mask and filtering the chunk\n",
    "        mask = (chunk['pickup_datetime'] > start_date) & (chunk['pickup_datetime'] < end_date)\n",
    "        \n",
    "        if mask.any():\n",
    "            df_list.append(chunk[mask][use_cols]) # Add the filtered chunk to the df_list\n",
    "            \n",
    "        pbar.set_description('Importing: %d' % (1 + i)) # More progress bar stuff?\n",
    "        pbar.update(1) # ditto\n",
    "        \n",
    "\n",
    "data = temp[:0].append(df_list) # This is actually the concat step, but we could have done the below too\n",
    "\n",
    "# data = pd.concat(df_list)\n",
    "\n",
    "del df_list # Free up memory \n",
    "\n",
    "data.to_csv(f'filtered_{filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f21dfd",
   "metadata": {},
   "source": [
    "### Uber data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512dea07",
   "metadata": {},
   "source": [
    "This file was smaller and did not have to be chunked during import, only filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "67780ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the Uber data\n",
    "\n",
    "uber = pd.read_csv('uber_apr2014.csv')\n",
    "\n",
    "uber['Date/Time'] = pd.to_datetime(uber['Date/Time'])\n",
    "uber_clean = uber[(uber['Date/Time'] > start_date) & (uber['Date/Time'] < end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e9aff932",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_clean.to_csv('filtered_uber.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
